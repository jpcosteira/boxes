{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211dd81-c118-4b8d-8ef8-cceb8b63f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as futures\n",
    "import grpc\n",
    "import grpc_reflection.v1alpha.reflection as grpc_reflection\n",
    "import logging\n",
    "import simplebox_pb2\n",
    "import simplebox_pb2_grpc\n",
    "import inspect\n",
    "import os\n",
    "import time\n",
    "    \n",
    "import io\n",
    "from scipy.io import loadmat, savemat\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "\n",
    "\n",
    "_PORT_ENV_VAR = 'PORT'\n",
    "_PORT_DEFAULT = 8061\n",
    "_ONE_DAY_IN_SECONDS = 60 * 60 * 24\n",
    "\n",
    "\n",
    "class ServiceImpl(simplebox_pb2_grpc.SimpleBoxServiceServicer):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            \n",
    "          Loads VGGT model \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    " \n",
    "    # Initialize the model and load the pretrained weights.\n",
    "    # This will automatically download the model weights the first time it's run, which may take a while.\n",
    "        self._model = VGGT.from_pretrained(\"facebook/VGGT-1B\").to(device)        \n",
    "        self._device = device\n",
    "\n",
    "\n",
    "\n",
    "    def process(self, request: simplebox_pb2.matfile, context):\n",
    "        \"\"\"\n",
    "        matfile is the matlab file with input data\n",
    "\n",
    "        Args:\n",
    "            request: The ImageAndFeatures request to process\n",
    "            context: Context of the gRPC call\n",
    "\n",
    "        Returns:\n",
    "            The Image with the applied function\n",
    "        features={'kp','desc'}\n",
    "        \"\"\"\n",
    "        datain = request.data\n",
    "\n",
    "        ret_file= run_codigo(datain, self._model,self._device)\n",
    "        return simplebox_pb2.matfile(data=ret_file)\n",
    "\n",
    "\n",
    "def run_codigo(datafile,model,device):\n",
    "    \"\"\"\n",
    "    Reads all variables from a MATLAB .mat file given a file pointer,\n",
    "    \n",
    "    Parameters:\n",
    "    file_pointer (file-like object): Opened .mat file in binary read mode.\n",
    "\n",
    "    Returns:\n",
    "    new_file_pointer (io.BytesIO): In-memory file-like object containing the cloned .mat file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # SPECIFIC CODE STARTS HERE\n",
    "\n",
    "    #Load the mat file using scipy.io.loadmat\n",
    "    mat_data=loadmat(io.BytesIO(datafile))\n",
    "\n",
    "        \n",
    "    # Extract and flatten the imgdata array\n",
    "    imgdata = mat_data['imgdata'].squeeze()\n",
    "\n",
    "    # List to hold in-memory files (as BytesIO) and their associated filenames\n",
    "    file_buffers = []\n",
    "    filenames = []\n",
    "\n",
    "    # Create in-memory files\n",
    "    for i, data in enumerate(imgdata):\n",
    "        #filename = f'image_{i+1}.jpg' uncomment if using regular files\n",
    "        # change next lines to save data on files and pass filenames in load_and_process_images\n",
    "        buffer = io.BytesIO()\n",
    "        buffer.write(data.flatten().tobytes())  # Write binary content to buffer\n",
    "        buffer.seek(0)  # Reset pointer to beginning for future reads\n",
    "        file_buffers.append(buffer)\n",
    "        filenames.append(filename)\n",
    "\n",
    "    images = load_and_preprocess_images(file_buffers).to(device)\n",
    "   # bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "    dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(dtype=dtype):\n",
    "        # Predict attributes including cameras, depth maps, and point maps.\n",
    "            predictions = model(images)\n",
    "\n",
    "    p={}\n",
    "    for k,v in predictions.items():\n",
    "        p[k]=v.cpu()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # SPECIFIC CODE ENDS HERE\n",
    "\n",
    "    f=io.BytesIO()\n",
    "    # WRITE RETURNING DATA the predictions dictionary\n",
    "    savemat(f,p)\n",
    "    return f.getvalue()\n",
    "\n",
    "def get_port():\n",
    "    \"\"\"\n",
    "    Parses the port where the server should listen\n",
    "    Exists the program if the environment variable\n",
    "    is not an int or the value is not positive\n",
    "\n",
    "    Returns:\n",
    "        The port where the server should listen or\n",
    "        None if an error occurred\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        server_port = int(os.getenv(_PORT_ENV_VAR, _PORT_DEFAULT))\n",
    "        if server_port <= 0:\n",
    "            logging.error('Port should be greater than 0')\n",
    "            return None\n",
    "        return server_port\n",
    "    except ValueError:\n",
    "        logging.exception('Unable to parse port')\n",
    "        return None\n",
    "\n",
    "def run_server(server):\n",
    "    \"\"\"Run the given server on the port defined\n",
    "    by the environment variables or the default port\n",
    "    if it is not defined\n",
    "\n",
    "    Args:\n",
    "        server: server to run\n",
    "\n",
    "    \"\"\"\n",
    "    port = get_port()\n",
    "    if not port:\n",
    "        return\n",
    "\n",
    "    target = f'[::]:{port}'\n",
    "    server.add_insecure_port(target)\n",
    "    server.start()\n",
    "    logging.info(f'''Server started at {target}''')\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(_ONE_DAY_IN_SECONDS)\n",
    "    except KeyboardInterrupt:\n",
    "        server.stop(0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6356b-15fc-4a06-b619-9255f12baa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = grpc.server(futures.ThreadPoolExecutor(),\n",
    "                         options= [('grpc.max_send_message_length', 512 * 1024 * 1024), \n",
    "                                   ('grpc.max_receive_message_length', 512 * 1024 * 1024)])\n",
    "simplebox_pb2_grpc.add_SimpleBoxServiceServicer_to_server(ServiceImpl(), server)\n",
    "service_names = (simplebox_pb2.DESCRIPTOR.services_by_name['SimpleBoxService'].full_name,grpc_reflection.SERVICE_NAME)\n",
    "\n",
    "grpc_reflection.enable_server_reflection(service_names, server)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b255e1-0217-4c53-96f1-0b259990fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_server(server)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611d224-e608-457d-9c88-2ab4cdbce010",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01591a9-31bc-4335-a63d-8d7de58a7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='[ %(levelname)s ] %(asctime)s (%(module)s) %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        level=logging.INFO)\n",
    "    #Create Server and add service\n",
    "    server = grpc.server(futures.ThreadPoolExecutor(),\n",
    "                         options= [('grpc.max_send_message_length', 512 * 1024 * 1024), \n",
    "                                   ('grpc.max_receive_message_length', 512 * 1024 * 1024)])\n",
    "    simplebox_pb2_grpc.add_SimpleBoxServiceServicer_to_server(\n",
    "        ServiceImpl(), server)\n",
    "\n",
    "    # Add reflection\n",
    "    service_names = (\n",
    "        simplebox_pb2.DESCRIPTOR.services_by_name['SimpleBoxService'].full_name,\n",
    "        grpc_reflection.SERVICE_NAME\n",
    "    )\n",
    "    grpc_reflection.enable_server_reflection(service_names, server)\n",
    "\n",
    "    run_server(server)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
